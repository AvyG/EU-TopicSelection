---
title: "Research Notebook"
author: "Avelyn Garcia, Stephan Pangaanbean, Chris Butcher"
date: "31-05-2023"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Topic Modeling of the EU State of the Union Speeches
## Research Notebook

The European Union (EU) is a conglomeration of __ states with a population of __ million people making it the __ most important economy in the world, behind countries like ___. For this position in the world stage, determining the priorities and concerns that the EU had or has is paramount to understand its internal and external policy and its impact.  

The State of the Union Address is an speech deliver by the President of the European Commission each September to the Parliament that "takes stock of the achievements of the past year and presents the priorities for the year ahead" [https://state-of-the-union.ec.europa.eu/index_en]. It was institutionalized in 2010 with the signed of the Framework Agreement on relations between the European Parliament and the European Commission - Annex IV(5) [https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2010:304:0047:0062:EN:PDF]. 

Up to 2023, 11 speeches have been delivered, four by former President of the Commission José Manuel Barroso (2010-2013), four by former President President Jean-Claude Juncker (2015-2018) and three by current President of the Commission Ursula von der Leyen (2020-2022) with her last speech for her current mandate schedule for this September. During European parliamentary elections, no address is deliver. 

The main objective of this research notebook is to explain and analysis the main topics of these speeches as they provide a transparent window into the priorities of the European politics and its changes across time.

This research notebook is organized in __ sections.

--------------------------------------------------------------------------------
Disclaimer: chatGPT 3.5/4.0 was used in this project. The main uses during the project were: 

- Advanced Search Engine
  We used chat GPT to ask question like:
  "Can you give me models for topic analysis?", "Can you give me the strengths and weaknesses of each model?", "and what does htmltools?", "Assume there is a [matching] vector with [the] main themes, how can I [concatenate] my vectors to the matrix?"
  or
  Including questions about specific errors encounter while coding. For example:
  "Why do I have this error
Warning: package ‘tidyverse’ was built under R version 4.2.3Warning: package ‘ggplot2’ was built under R version 4.2.3Error: package or namespace load failed for ‘tidyverse’:[...], [What is happening?]"
   or
  "I get the following error
Error: '\s' is an unrecognized escape in character string starting ""^(STATE OF THE    UNION|State of the Union) [0-9]{4}[\n\s""
  
- Code Copilot
  In some instances, we used chatGPT to suggest how to perform a task or how improve existing code. For example:
  "and if I [want to] save multiple attributes from my webpage like name, link, description block if exists[,] how can I do it?"
  or
  "I am trying to remove the following text from different speeches [give an example of the text] can you give me a regular expression that eliminate this strings"
  or
  "I am using pdf_speech <- sub("^\\s*State of the Union [0-9]{4}\\s*[\\sA-Za-z\\s-]*",     "----------", pdf_speech) but a better Europe does not count in my regular 


The text in between [] are either grammatical corrections or adding text to clarify the example. If there are any question, please send an email to avelynfernanda.garciaaraya@student.kuleuven.be 

--------------------------------------------------------------------------------

#### Libraries

This are the libraries our project used

```{r libraries}
# Decomment this if you want to install any of the packages
#install.packages(c("rvest","tidyverse","httr","pdftools","quanteda", "wordcloud","topicmodels","textstem"))

library(httr)
library(pdftools)
library(rvest)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(wordcloud)
library(topicmodels)
library(textstem)

```

#### Data Colection

For our project we will concentrate in two sources of data

- The keywords associated with each speech. These keywords are scraped from the official website. We want these keywords for validation purposes.

- The pdf that has the speech. For now, it was for the best to manually download each speech to ensure that: first, they were in English and second, they are the correct speech (for example, a particular case we encounter is accessing the speech of 2020. If one would try to see the speech online, the link will redirect you to the 2021 speech).

In this section, we will scrap the main topic keywords associated with each speech. First we will scrap the keywords of the speeches associated with the current president to then scrap the keywords of the speeches associated with the other two presidents.


```{r crawling identification}

# We identify ourselves

httr::set_config(httr::user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:113.0.2) Gecko/20100101 Firefox/113.0.2 (scrapping EU speeches and keywords for a homework <3, thank you for your patience)"))

```


```{r keywords_current_president1}

# Specify the original url from which we will index and scrap the speeches
base_url <- 'https://state-of-the-union.ec.europa.eu/index_en'

webpage <- read_html(base_url)

```


```{r keywords_current_president2, echo=FALSE}

# Extract the key words from the speeches of the current president
name_speech <- webpage %>% 
              html_nodes("h1.ecl-content-block__title") %>%
              html_nodes(".ecl-link.ecl-link--standalone") %>%
              html_text()
name_speech <- head(name_speech,-5)

links <- webpage %>% 
              html_nodes("h1.ecl-content-block__title") %>%
              html_nodes(".ecl-link.ecl-link--standalone") %>%
              html_attr("href")
links <- head(links,-3)

description_speech <- webpage %>% 
                      html_nodes("div.ecl-content-block__description") %>%
                      html_nodes(".ecl") %>%
                      html_text()

df <- data.frame(name_speech = name_speech, main_topics = description_speech)
df

```


``` {r keywords_previous_presidents1}

# Extract the key words from the speeches of previous presidents
# To be a good bot, we add a small delay time, even if https://ec.europa.eu/robots.txt and https://state-of-the-union.ec.europa.eu/robots.txt do not indicate the necessity of one.

base_url <- 'https://state-of-the-union.ec.europa.eu'
full_urls <- paste(base_url, tail(links,2),sep='')

pages <- lapply(full_urls, function(u) {
   wp <- read_html(u)
   Sys.sleep(11)
   return(wp)
})

```

```{r keywords_previous_presidents2}

# Get the speech keywords from the speeches of the previous two presidents (Run the cell altogether)

# Find the components of interest
descriptions_speech <- lapply(pages, function(pages) html_nodes(pages, "div.ecl p"))
names_speech <- lapply(pages, function(pages) html_nodes(pages, "div.ecl h3"))

# Convert them into text
descriptions_speech <- lapply(descriptions_speech, html_text)
names_speech <- lapply(names_speech, html_text)

# Clean the element of description (there is extra info)
descriptions_speech <- lapply(descriptions_speech, function(x) x[!grepl("Address \\| Video", x)])

# Unlist element to save them in a ddf
descriptions_speech <- unlist(descriptions_speech)
names_speech <- unlist(names_speech)

# And add these keywords to a matrix to save them
temp_df <- data.frame(name_speech = names_speech, main_topics = descriptions_speech)
df <- bind_rows(df, temp_df)

# Create new columns year and speech_text
df <- df %>% mutate(year = sub(".*\\b(\\d{4})$", "\\1", name_speech),
                    speech_text = NA)
df
```

```{r save data}
# We save the data to avoid calling the scrapper all the time
save.image("df_keywords.RData")

```

We can see that there has been a evolution of main topics across time. The first speeches are mainly about the economic shape of the European union, to then talk mainly about economic recuperation mixed with the migration crisis and investment across multiple industries to finally mainly talk about COVID19, health and the Ukraine war.


#### Preprocessing

First, as mention above we manually downloaded the PDF of interest (which are together with this notebook). The reason for that is to ensure the quality and appropriateness of our text. Equally mentioned above, during our scrapping we encounter that some links that were wrongly linked or the speech proposed were in other languages besides English.

We notice from the pdfs that some irrelevant information is readable depending on the format of the document. This extra information variates across speeches. Some examples of this irrelevant information for our purposes are:

- The cover of some speeches
- The page number
- Titles and subtitles
- Preambles
- Images
- Finishers

We try clean the text so we have as much of the spoken speech as possible

```{r readpdfs}
path_to_folder <- "./PDFs/"

pdf_files <- list.files(path = path_to_folder, pattern = "\\.pdf", full.names = TRUE)

for (pdf_file in pdf_files){
  pdf_speech <- pdf_text(pdf_file)
  par_year <- regmatches(pdf_speech, regexpr("September\\s*(\\d{4})|STATE OF THE UNION\\s*(\\d{4})", pdf_speech))
  par_year <- par_year[1]
  par_year <- str_sub(par_year,-4,-1)

  # pdf_text returns an element for each page, we already know that the 2016 document include more texts than the speech. Even if we can erase the rest of the text in a later step, for simplicity we will do it now
  
  if(par_year == "2016"){pdf_speech <- pdf_speech[1:22]}
  
  # Clean the page number (easier to do it before the merging of pages)
  pdf_speech <- sub("\\s*\\d+\\s*(\\n)+$", " ", pdf_speech)
  
  # Clean the header if exist
  pdf_speech <- sub("^\\s*State of the Union [0-9]{4}\\s*.*?\\n{4,}", "\n", pdf_speech, ignore.case = TRUE, perl = TRUE)
  
  # Collapse the pages
  speech_joint <- paste(pdf_speech, collapse =" ")
  
  # Clean 's
  speech_joint <- gsub("’s|'s", " ", speech_joint, ignore.case = TRUE)
  
  # Clean titles (2015 is not possible because our expression eraised part of the speech)
  if (par_year %in% c("2012")){
    speech_joint <- gsub("\\n{2,}\\d+\\s*\\.[A-Za-z:\\s'\\-\\—\\–]+\\n|\\n{2,}\\s*[a-z]\\) [A-Za-z\\s:'\\-\\—\\–]+\\n", "\n", speech_joint, perl = TRUE)
    }
  else if (par_year %in% c("2016","2017","2018","2020")){
    speech_joint <- gsub("\\n{1,}[A-Z:\\-\\—\\–\\s]+\n{1,}", "\n", speech_joint, perl = TRUE) 
  }
  else if (par_year %in% c("2021","2022")){
    speech_joint <- gsub("\\n{3,}[A-Z][A-Za-z\\s]+\\n{1,}", "\n", speech_joint, perl = TRUE) 
  }
  
  # Clean punctuation
  speech_joint <- gsub("[[:punct:]]", " ", speech_joint)
  
  # Clean preamble
  speech_joint <- sub(".*?(Mr President|Dear President|President \nHonourable Members|Mr  President)", " ", speech_joint)

  # Clean finisher
  speech_joint <- sub("(SPEECH|Updated version following delivery|For further information| Jean Claude Juncker).*", "", speech_joint)
  
  # Clean repetitive addresses to public
  speech_joint <- gsub("Presidency of the Council|Madam President of the Council|Dear Mr President|Dear President|Mr President|Madam President|Honourable Members of the European Parliament|Honourable Members|Honourable members|Minister|Ladies and Gentlemen|Ladies and gentlemen|My fellow Europeans|My dear colleagues", " ", speech_joint)
  
  # Clean spaces
  speech_joint <- gsub("\\s+", " ", speech_joint)
  
  # Match the speech with its year
  df$speech_text[df$year == par_year] <- speech_joint
  
}

df <- as_tibble(df)
df

```

To finish preprocessing our texts, we tokenize, lemmatize and remove stopwords to improve the results.


```{r textstem}

custom_stopwords = c(stopwords("en"), "europe", "european", "also", "much")

my_tokens <- df$speech_text %>%  tolower() %>%
                        lemmatize_strings() %>%
                        corpus() %>%
                        quanteda::tokens() %>%
                        quanteda::tokens_remove(custom_stopwords)

length(my_tokens)

```

### Descriptive Analysis

```{r}

speeches <- corpus(clean_df$tokens_lemma[1]) %>%
            quanteda::tokens() 

textstat_frequency(speeches)[c(1:100), ]

speeches

str(clean_df$speech_text[1])

str(clean_df$tokens_lemma[1])


```



We would like to start with some descriptive analysis of the speech. First we use a cloud of words for each speech to see what are the most common words.

```{r wordcloud for each speech}

# Assuming df is your data frame and speech_text is the column containing speeches
for (i in 1:length(my_tokens)){
  dfm_matrix <- my_tokens[i] %>% dfm()
  textplot_wordcloud(dfm_matrix, max_words = 35)
}


dfm_matrix <- my_tokens %>% dfm()
dfm_matrix 
  # Create a dfm for each speech
  dfm_matrix1 <- dfm(tokens1)
  
  # Convert the dfm to a data frame
  dfm_df1 <- convert(dfm_matrix1, to = "data.frame")
  
  # Calculate word frequencies
  dfm_freq1 <- textstat_frequency(dfm_matrix1)

  
  # Plot a word cloud for each speech
  textplot_wordcloud(dfm_matrix, max_words = 35)


```
# Conclusion!




```{r savecsv}
# Save the data 
save.image("df_keywords.RData")

```





#### Text Analysis (Topic modeling)

Look for more models

```{r LDA}
# LDA, we need to individualize the topics searching

for (i in 1:nrow(clean_df)){
  dfm_matrix <- #corpus(df$clean_tokens[i]) %>%
                quanteda::corpus(clean_df$tokens_lemma[i]) %>%
                quanteda::tokens() %>%
                dfm()
  
  lda <- dfm_matrix %>% convert(to = "topicmodels") %>% 
         LDA(k=5, control = list(seed=0, alpha=1/c(1:10)))

  print(terms(lda))
}


 
```



```{r}

```


```{r}

```
```{r}

```
```{r}

```




#### Text Analysis (Validation)

To validate we have three roads:
- Compare multiple models
- Compare each model with the keywords from the website
- Manual checking

```{r}

```



#### Conclusions

()






